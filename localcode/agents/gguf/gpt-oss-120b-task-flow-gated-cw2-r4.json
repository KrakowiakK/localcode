{
  "name": "gguf/gpt-oss-120b-task-flow-gated-cw2-r4",
  "description": "GPT-OSS 120B flow profile (tool-gated stages, context window 2)",
  "prompt": "prompts/gpt-oss-120b.txt",
  "model": "gpt-oss-120b-mxfp4",
  "url": "http://localhost:1235/v1/chat/completions",
  "temperature": 1,
  "top_p": 1,
  "top_k": null,
  "min_p": null,
  "max_tokens": 16000,
  "tool_choice": "auto",
  "cache": true,
  "think": true,
  "think_level": "medium",
  "tool_name_style": "alias",
  "auto_tool_call_on_failure": true,
  "require_code_change": true,
  "min_tool_calls": 1,
  "max_format_retries": 2,
  "max_batch_tool_calls": 1,
  "task_branching": false,
  "tools": [
    "list_dir",
    "read_file",
    "read_files",
    "write_file",
    "replace_in_file",
    "patch_files",
    "find_files",
    "search_text",
    "flow_stage_done"
  ],
  "native_thinking": true,
  "flow": [
    {
      "id": "context",
      "label": "CONTEXT",
      "prompt": "Gather context: identify relevant files, constraints, and current behavior. Use read/search tools as needed.",
      "require_code_change": false,
      "history_mode": "tail",
      "history_max_messages": 6,
      "history_keep_first": true,
      "require_files_read": 1,
      "require_non_flow_tool": 1,
      "allow_missing_done": true,
      "tools_allow": ["list_dir", "read_file", "read_files", "find_files", "search_text"]
    },
    {
      "id": "architect",
      "label": "ARCHITECT",
      "prompt": "Propose approach and risks. Decide what will change and why. No code changes.",
      "require_code_change": false,
      "history_mode": "tail",
      "history_max_messages": 6,
      "history_keep_first": true,
      "allow_missing_done": true,
      "tools_allow": ["list_dir", "read_file", "read_files", "find_files", "search_text"]
    },
    {
      "id": "implement",
      "label": "IMPLEMENT",
      "prompt": "Implement the changes. Use tools to edit files and verify if needed.",
      "require_code_change": true,
      "history_mode": "tail",
      "history_max_messages": 14,
      "history_keep_first": true,
      "tools_allow": ["list_dir", "read_file", "read_files", "find_files", "search_text", "write_file", "replace_in_file", "patch_files", "flow_stage_done"]
    }
  ],
  "flow_stage_retries": 2,
  "flow_stage_required": true,
  "flow_history_mode": "tail",
  "flow_history_max_messages": 8,
  "flow_history_keep_first": true,
  "flow_context_window": 2,
  "server_config": {
    "model_path": "~/.lmstudio/models/ggml-org/gpt-oss-120b-GGUF/gpt-oss-120b-mxfp4-00001-of-00003.gguf",
    "context_window": 131072,
    "extra_args": [
      "--swa-full",
      "--cache-type-k",
      "f16",
      "--cache-type-v",
      "f16",
      "--ubatch-size",
      "2048",
      "--chat-template-file",
      "llama.cpp/models/templates/openai-gpt-oss-120b.jinja",
      "--jinja",
      "--reasoning-format",
      "auto"
    ],
    "hf_model": "hf://ggml-org/gpt-oss-120b-GGUF/gpt-oss-120b-mxfp4-00001-of-00003.gguf"
  }
}
