{
  "name": "gguf/qwen3-coder-next-mxfp4",
  "description": "Qwen3 Coder Next (80B/A3B) MXFP4 MoE via llama.cpp â€” coding-optimized, non-thinking, noctrex GGUF.",
  "prompt": "prompts/qwen3-coder.txt",
  "model": "qwen3-coder-next-mxfp4",
  "url": "http://localhost:1235/v1/chat/completions",
  "temperature": 0.3,
  "top_p": 0.95,
  "top_k": 40,
  "min_p": 0.01,
  "presence_penalty": 0.1,
  "frequency_penalty": 0.1,
  "max_tokens": 16000,
  "tool_choice": "auto",
  "cache": false,
  "think": false,
  "tool_name_style": "alias",
  "auto_tool_call_on_failure": true,
  "require_code_change": true,
  "thinking_visibility": "show",
  "min_tool_calls": 1,
  "max_turns": 12,
  "max_format_retries": 2,
  "max_batch_tool_calls": 1,
  "tools": [
    "find_files",
    "read_file",
    "write_file"
  ],
  "native_thinking": false,
  "server_config": {
    "model_path": "~/.lmstudio/models/noctrex/Qwen3-Coder-Next-MXFP4_MOE-GGUF/Qwen3-Coder-Next-MXFP4_MOE.gguf",
    "context_window": 32768,
    "extra_args": [
      "--jinja",
      "--no-context-shift",
      "--fit",
      "on",
      "-fa",
      "on",
      "-b",
      "2048",
      "-ub",
      "2048",
      "--cache-type-k",
      "q8_0",
      "--cache-type-v",
      "q8_0",
      "--reasoning-format",
      "none",
      "--parallel",
      "1",
      "--mlock",
      "--no-mmap"
    ],
    "hf_model": "hf://noctrex/Qwen3-Coder-Next-MXFP4_MOE-GGUF/Qwen3-Coder-Next-MXFP4_MOE.gguf"
  }
}
