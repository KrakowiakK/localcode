{
  "name": "gguf/qwen3-next-80b-q8",
  "description": "Qwen3 Next 80B (A3B active) Q8_0 via llama.cpp â€” near-lossless quantization, non-thinking instruct model.",
  "prompt": "prompts/qwen3.txt",
  "model": "qwen3-next-80b-q8",
  "url": "http://localhost:1235/v1/chat/completions",
  "temperature": 1.0,
  "top_p": 0.95,
  "top_k": 40,
  "min_p": 0,
  "presence_penalty": 0.01,
  "max_tokens": 16000,
  "tool_choice": "auto",
  "cache": false,
  "tool_name_style": "canonical",
  "auto_tool_call_on_failure": true,
  "require_code_change": true,
  "min_tool_calls": 1,
  "max_turns": 24,
  "max_format_retries": 2,
  "max_batch_tool_calls": 1,
  "history_max_messages": 16,
  "tools": [
    "ls",
    "read",
    "write",
    "edit",
    "glob",
    "grep",
    "search",
    "apply_patch",
    "finish"
  ],
  "server_config": {
    "model_path": "~/.lmstudio/models/ggml-org/Qwen3-Next-80B-A3B-Instruct-GGUF/Qwen3-Next-80B-A3B-Instruct-Q8_0-00001-of-00003.gguf",
    "context_window": 131072,
    "extra_args": [
      "--jinja",
      "--no-context-shift",
      "--fit",
      "on",
      "--cache-type-k",
      "f16",
      "--cache-type-v",
      "f16",
      "--reasoning-format",
      "none",
      "--parallel",
      "1",
      "--mlock",
      "--no-mmap"
    ],
    "hf_model": "hf://ggml-org/Qwen3-Next-80B-A3B-Instruct-GGUF/Qwen3-Next-80B-A3B-Instruct-Q8_0-00001-of-00003.gguf"
  }
}
