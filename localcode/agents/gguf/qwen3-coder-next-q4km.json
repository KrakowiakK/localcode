{
  "name": "gguf/qwen3-coder-next-q4km",
  "description": "Qwen3 Coder Next (80B/A3B) Q4_K_M MoE via llama.cpp â€” hybrid Mamba-Attention, 512 experts top-10.",
  "prompt": "prompts/qwen3-coder.txt",
  "model": "qwen3-coder-next-q4km",
  "url": "http://127.0.0.1:1235/v1/chat/completions",
  "temperature": 0.0,
  "top_p": 0.95,
  "top_k": 40,
  "min_p": 0.01,
  "presence_penalty": 0.1,
  "frequency_penalty": 0.1,
  "max_tokens": 4096,
  "request_timeout": 600,
  "tool_choice": "required",
  "cache": false,
  "tool_name_style": "canonical",
  "auto_tool_call_on_failure": true,
  "require_code_change": true,
  "send_tool_categories": false,
  "min_tool_calls": 1,
  "max_turns": 24,
  "max_format_retries": 2,
  "max_batch_tool_calls": 1,
  "history_max_messages": 16,
  "tools": [
    "ls",
    "read",
    "write",
    "edit",
    "glob",
    "grep",
    "search",
    "apply_patch",
    "finish"
  ],
  "server_config": {
    "model_path": "~/.lmstudio/models/lmstudio-community/Qwen3-Coder-Next-GGUF/Qwen3-Coder-Next-Q4_K_M.gguf",
    "context_window": 32768,
    "extra_args": [
      "--host", "0.0.0.0",
      "--jinja",
      "--no-context-shift",
      "--flash-attn", "on",
      "--batch-size", "2048",
      "--ubatch-size", "2048",
      "--cache-type-k", "f16",
      "--cache-type-v", "f16",
      "--reasoning-format", "none",
      "--parallel", "1",
      "--mlock"
    ]
  }
}
