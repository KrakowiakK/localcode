{
  "name": "gguf/step-3.5-flash-q4km",
  "description": "StepFun Step 3.5 Flash (196B/11B MoE) Q4_K_M via llama.cpp â€” MTP-1 speculative decode.",
  "prompt": "prompts/step-3.5-flash.txt",
  "model": "step-3.5-flash-q4km",
  "url": "http://localhost:1235/v1/chat/completions",
  "temperature": 1.0,
  "top_p": 0.95,
  "top_k": 0,
  "min_p": 0.01,
  "max_tokens": 16384,
  "request_timeout": 600,
  "tool_choice": "auto",
  "cache": false,
  "tool_name_style": "canonical",
  "auto_tool_call_on_failure": true,
  "require_code_change": true,
  "send_tool_categories": false,
  "min_tool_calls": 1,
  "max_turns": 24,
  "max_format_retries": 2,
  "max_batch_tool_calls": 1,
  "history_max_messages": 16,
  "tools": [
    "ls",
    "read",
    "write",
    "edit",
    "glob",
    "grep",
    "search",
    "apply_patch",
    "finish"
  ],
  "server_config": {
    "model_path": "~/Desktop/BENCHMARK/llama.cpp/_moved_from_localcode_root_20260217/models/Step-3.5-Flash-Q4_K_M.gguf",
    "context_window": 32768,
    "extra_args": [
      "--jinja",
      "--no-context-shift",
      "--fit", "on",
      "-fa", "on",
      "-b", "8192",
      "-ub", "1024",
      "--cache-type-k", "q8_0",
      "--cache-type-v", "q8_0",
      "--reasoning-format", "deepseek",
      "--parallel", "1",
      "--mlock",
      "--no-mmap",
      "--draft-max", "1"
    ]
  }
}
